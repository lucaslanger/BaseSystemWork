\section{Learning Algorithm for M-PSR}

This section describes a learning algorithm for M-PSR combining standard spectral techniques~\cite{bootspsr} with a algorithm for building an extended set of symbols $\Sigma'$ containing frequent patterns of observations.

\subsection{Spectral Learning Algorithm}

We start by extending the spectral learning algorithm to M-PSR under the assumption that $\kappa$ and $\Sigma'$ are known. In this case, the learning procedure only needs to recover the operators $\A_\sigma$ for all $\sigma \in \Sigma'$, and the initial and final weights $\aone, \ainf$. We first recall some basic notation about Hankel matrices~\cite{CarlylePaz71,Fliess74}, and then proceed to describe the learning algorithm. To simplify the description of the learning algorithm we assume that the function $f : \sstar \to \R$ associated with the target M-PSR can be evaluated for every string. In practice these are unknown, but can be effectively estimated from data because they correspond to probabilities of observations. 

Given $f : \sstar \to \R$, we will use its \emph{Hankel matrix} representation $\H_f \in \R^{\sstar \times \sstar}$, which is an infinite matrix with rows and columns indexed by strings in $\sstar$ and whose entries satisfy $\H_f(u,v) = f(u v)$. To efficiently work with this matrix, we only consider finite sub-blocks indexed by a finite set of prefixes $\Ps \subset \sstar$ and suffixes $\Ss \subset \sstar$. Both $\Ps$ and $\Ss$ are input parameters given to the algorithm; see \cite{icml12} for a discussion on how to choose these in practice. The pair $\Bs = (\Ps,\Ss)$ is called a basis, and it determines a sub-block $\H_\Bs \in \R^{\Ps \times \Ss}$ of $\H_f$ with entries given by $\H_\Bs(u,v) = \H_f(u,v)$ for all $u \in \Ps$ and $v \in \Ss$. For a fixed basis, we also consider the vectors $\mat{h}_{\Ss} \in \R^{\Ss}$ with entries given by $\mat{h}_{\Ss}(v) = \H_f(\epsilon,v)$ for every $v \in \Ss$, and $\mat{h}_{\Ps} \in \R^{\Ps}$ with $\mat{h}_{\Ps}(u) = \H_f(u,\epsilon)$.

Note that the definitions above only depend on $\Sigma$. In order to recover operators $\A_\sigma$ for all $\sigma \in \Sigma'$ we need to consider multi-step shifts of the finite Hankel matrix $\H_{\Bs}$. In particular, given $\sigma \in \Sigma'$ we define the sub-block $\H_\sigma \in \R^{\Ps \times \Ss}$ whose entries are given by $\H_\sigma(u,v) = f(u \, \sigma \, v)$. This can be interpreted as either using the lift $f(\kappa(u) \, \sigma \, \kappa(v))$ or the decoding $f(u \, \partial(\sigma) \, v)$, but the actual value in the matrix $\H_\sigma$ will be the same.

Using the notation above we can give a simple description of the learning algorithm. Suppose the basis $\Bs$ and the desired number of states $n$ are given. The algorithm starts by collecting a set of sampled trajectories and uses them to estimate the matrices $\H_{\Bs}, \H_\sigma \in \R^{\Ps \times \Ss}$ and vectors $\mat{h}_{\Ps} \in \R^{\Ps}$, $\mat{h}_{\Ss} \in \R^{\Ss}$. Then, it takes the truncated SVD $\mat{U}_n \mat{D}_n \mat{V}_n^\top$ of $\H_{\Bs}$, where $\mat{D}_n \in \R^{n \times n}$ contains the first $n$ singular values of $\H_{\Bs}$, and $\mat{U}_n \in \R^{\Ps \times n}$ and $\mat{V}_n \in \R^{\Ss \times n}$ contain the first left and right singular vectors respectively. Finally, it computes the transition operators of the M-PSR as $\A_\sigma = \mat{D}_n^{-1} \mat{U}_n^\top \H_\sigma \mat{V}_n$ for all $\sigma \in \Sigma'$, and the initial and final weights as $\aone^\top = \mat{h}_{\Ss}^\top \mat{V}_n$ and $\ainf = \mat{D}_n^{-1} \mat{U}_n^\top \mat{h}_{\Ps}$.
%
This algorithm yields an M-PSR with $n$ states. It was proved in~\cite{bootspsr} that this algorithm is statistically consistent for PSRs (under a mild condition on $\Bs$). The same guarantees trivially extend to M-PSR.

%\borja{Need to fill this. Probably copy-paste from the ODM paper will do.}
%
%\borja{BEGIN OF COPY-PASTE. NEEDS EDITS}
%
%A convenient algebraic way to summarize all the information conveyed by $f$ is
%with its \emph{Hankel matrix}, a bi-infinite matrix $\H_f \in \R^{\Sstar \times \Sstar}$ with rows and
%columns indexed by strings in $\Sstar$.
%%
%Strings indexing rows and columns are interpreted as prefixes and suffixes respectively.
%The entries in $\H_f$ are given by $\H_f(u,v) = f(u,v)$ for every $u, v
%\in \Sstar$.
%
%% THIS IS FOR THE SDM
%%$\H_f(u,v) = f_u(v) = f(u v) / f(u)$,
%%\textbf{TODO (Borja):} Beware the case $f(u) = 0$ !!
%
%Although $\H_f$ is an infinite matrix, in some cases it can have finite rank.
%%
%In particular, a well-known result states that $\H_f$ has rank at most $n$ if
%and only if  there exists a PSR $\psrA$ with $n$ states satisfying
%$f_{\psrA} = f$ \cite{CarlylePaz71,Fliess74}.
%%
%This result is the basis of recently developed spectral learning algorithms for
%PSRs \cite{bootspsr}, which we review in Sec.~\ref{sec:learning}.
%
%Instead of looking at the full Hankel matrix, algorithms usually work with finite
%sub-blocks of this matrix.
%%
%A convenient way to specify such blocks is to give the ``names'' to the rows and
%columns.
%%
%Specifically, given a finite set of prefixes $\Ps \subset \Sstar$ and a finite
%set of suffixes $\Ss \subset \Sstar$, the pair $\Bs = (\Ps,\Ss)$ is
%a \emph{basis} defining the sub-block $\H_\Bs \in \R^{\Ps \times \Ss}$ of
%$\H_f$, whose entries are given by $\H_\Bs(u,v) = \H_f(u,v)$.
%% for every $u \in
%%\Ps$ and $v \in \Ss$.
%%
%Note that every sub-block built in this way satisfies $\rank(\H_\Bs) \leq
%\rank(\H_f)$; when equality is attained, the basis $\Bs$ is
%\emph{complete}.
%
%Sometimes it is also convenient to look at one-step shifts of the finite
%Hankel matrices.
%%
%Let $\H \in \R^{\Ps \times \Ss}$ be a finite sub-block of
%$\H_f$ specified by a basis $\Bs = (\Ps,\Ss)$.
%%
%Then, for every symbol $\sigma \in \Sigma$, we define the sub-block $\H_\sigma \in
%\R^{\Ps \times \Ss}$ whose entries are given by $\H_\sigma(u,v) = \H_f(u,\sigma
%v)$.
%%
%For a fixed basis, we also consider the vectors $\mat{h}_{\Ss}
%\in \R^{\Ss}$ with entries given by $\mat{h}_{\Ss}(v) = \H_f(\lambda,v)$
%for every $v \in \Ss$, and $\mat{h}_{\Ps} \in \R^{\Ps}$ with $\mat{h}_{\Ps}(u) =
%\H_f(u,\lambda)$.
%
%The Hankel matrix $\H_f$ is tightly related to the \emph{system dynamics matrix}
%(SDM) of the stochastic process described by $f$~\cite{singh04}, but while the entries of the
%Hankel matrix represent \emph{joint} probabilities over prefixes and suffixes,
%the corresponding entry in the SDM is the \emph{conditional} probability of
%observing a suffix given the prefix.
%
%
%The algorithm takes as input $\Sigma$ and a basis
%$\Bs$ in $\Sstar$, uses them to estimate the corresponding Hankel matrices, and
%then recovers a PSR by performing singular value decomposition and
%linear algebra operations on these matrices.
%%
%%Estimating Hankel matrices containing probabilities of observed trajectories
%%from a sample is straightforward, and the details of the spectral learning
%%algorithm are reviewed below.
%%
%Although the method works almost out-of-the-box, in practice the results tend
%to be sensitive to the choice of basis.
%%
%Thus, after briefly recapitulating how the spectral learning algorithm proceeds,
%we will devote the rest of the section to describe a procedure for building a
%basis which is tailored for the case of learning option duration models.
%
%%\textbf{TODO: Now we should talk in here about Hankel matrices}
%
%Suppose the basis $\Bs$ is fixed and the desired number of states $n$ is given.
%%
%Suppose that a set of sampled trajectories was used to estimate the
%Hankel matrices $\H, \H_\sigma \in \R^{\Ps \times \Ss}$ and vectors
%$\mat{h}_{\Ps} \in \R^{\Ps}$, $\mat{h}_{\Ss} \in \R^{\Ss}$ defined in
%Sec.~\ref{sec:hankel}.
%%
%The algorithm starts by taking the truncated SVD $\mat{U}_n \mat{D}_n
%\mat{V}_n^\top$ of $\H$, where $\mat{D}_n \in \R^{n \times n}$ contains the first
%$n$ singular values of $\H$, and $\mat{U}_n \in \R^{\Ps \times n}$ and $\mat{V}_n
%\in \R^{\Ss \times n}$ contain the first left and right singular vectors
%respectively.
%%
%Finally, we compute the transition operators of a PSR as $\A_\sigma =
%\mat{D}_n^{-1} \mat{U}_n^\top \H_\sigma \mat{V}_n$, and the initial and final
%weights as $\aone^\top = \mat{h}_{\Ss}^\top \mat{V}_n$ and $\ainf =
%\mat{D}_n^{-1} \mat{U}_n^\top \mat{h}_{\Ps}$.
%%
%This yields a PSR with $n$ states. It was proved in~\cite{bootspsr} this
%algorithm is statistically consistent: if the population Hankel matrices are
%known and the basis $\Bs$ is complete, then the learned PSR is equivalent to the
%one that generated the data.
%
%
%\borja{END OF COPY-PASTE}

\subsection{A Generic Coding Function}

%Here we provide a dynamic programming algorithm which can serve as $\kappa$ for any M-PSR. Given a query string Q, and a set of transition sequences $\Sigma'$, the algorithm minimizes the number of sequences used in the partition $\kappa(Q)$. In other words, the algorithm minimizes $|\kappa(Q)|$ over all possible encodings of Q. For the single observation case, the algorithm is equivalent to the coin change problem.
%
%For a given string Q, the algorithm inductively computes the optimal string encoding for the prefix Q[:i]. It does so by minimizing over all $s \in \Sigma'$ which terminate at the index i of Q. We provide the full pseudo code for the encoding function in the appendix.

Given $\Sigma$ and $\Sigma'$, a generic coding function $\kappa : \sstar \to {\Sigma'}^\star$ can be obtained by minimizing the coding length $|\kappa(x)|$ of every string $x \in \sstar$. More formally, we consider the coding $\kappa$ given by

\begin{equation*}
\kappa(x) ~= \argmin_{ z \in \Sigma', \; x=yz, \; |y|<|x|} |\kappa(y)(z)| \enspace 
\end{equation*}

%\begin{equation*}
%\kappa(x) = \argmin_{y \in {\Sigma'}^\star, \; \partial(y) %= x} |y| \enspace.
%\end{equation*}
Note that for the single observation case, this is equivalent to the optimal coin change problem, which is a textbook example of dynamic programming. This has advantage of minimizing the number of operators $\A_\sigma$ that will need to be multiplied to compute the value of the M-PSR on a string $x$. At the same time, operators expressing long transition sequences can capture the contributions of all intermediate states even if the chosen model size is too small to represent every state traversed by a single-observation model. This is one of the reasons why M-PSRs show better performance than PSR for smaller models (see Section~\ref{sec:exp}).

%\lucas{Feel free to edit the last two sentences}

%This is a significant advantage if one works with a smaller number of states. To demonstrate this intuition we contrast the computation of the operators with a PSR with an M-PSR.


%At the same time, if the operators $\A_\sigma$ are noisy due to the learning process, multiplying a small number of them should also be beneficial in terms of minimising the amount of error accumulated in these multiplications.
%

The pseudocode of the algorithm is given in Algorithm 1. It inductively computes the optimal string encoding for the prefix $x_1 \cdots x_i$ for all $1 \leq i \leq |x|$. This can be obtained by minimizing over all $\sigma \in \Sigma'$ which terminate at the index $i$ of $x$.
We use the following notation:

\textbf{bestEncoding}: A map from indices $i$ of the query string $x$ to the optimal encoding of $x[:i]$.

\textbf{minEncoding}: A map from indices $i$ of the query string $x$ to $|bestEncoding[i]|$

\textbf{opsEnding}: A map from indices $i$ of $x$ to the set of strings in $\Sigma'$: $\{s \in \Sigma',$ $x[i-|s|:i] = s\}$

\algnewcommand\algorithmicinput{\textbf{INPUT:}}
\algnewcommand\algorithmicoutput{\textbf{OUTPUT:}}

\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\OUTPUT{\item[\algorithmicoutput]}

\begin{algorithm}
\caption{Encoding Algorithm}
\label{Encoding Algorithm}
\begin{algorithmic}[1]
\INPUT $x$
\OUTPUT $\kappa(x)$

\Procedure{DPEncode}{}

\State $bestEncoding[] \gets String[|x|+1]$
\State $minEncoding[] \gets Int[|x|+1]$
\State $opsEnding[] \gets String[|x|+1][]$

\State $bestEncoding[0] = x[0]$
\State $minEncoding[0] = 0$

\For{i in $[1,|x|]$}
	 \State $ospEnding[i] \gets \{s \in \Sigma', x[i-|s|:i] = s\}$
\EndFor

\For{i in $[1,|x|]$}
	\State $bestOp \gets null$
	\State $m \gets 0$ 
	\For{$s \in opEnd[i]$}
		\State $t \gets minE[i-|s|] + 1$
		\If{$bestOp = null$ or $t < m$}
			\State $m \gets t$ 
			\State $bestOp \gets s$
		\EndIf
	\EndFor
	\State $minEncoding[i+1] \gets m$
	\State $bestEncoding[i+1] \gets bestEncoding[i-|bestOp|] + bestOp$
\EndFor

\State \Return $bestEncoding[|x|]$

\EndProcedure
\end{algorithmic}
\end{algorithm}

%We provide the full pseudo-code for this encoding function in the first section of the appendix.

\subsection{State Update}

When using classical PSR in online environments one typically updates the state vector every time a new observations is available. This eliminates the need for repeatedly transforming the initial state over the whole history of observations $h$ when making predictions. In the case of M-PSRs, the dynamic programming algorithm for minimizing the length of string encodings provides a naturally convenient way to perform efficient state updates. To do so, we cache past state vectors along with their encoding length. When a new observation $\sigma$ is read, we determine the encoding for the extended observation string $h \sigma$ with the same recurrence relation as in the previous section. Because this minimization is over $\{s \in \Sigma' \,|\, \exists p \in \Sigma^* \, ps= h\sigma\}$, one needs to cache at most $\max_{s \in \Sigma'} |s|$ state vectors and encoding lengths. 

\subsection{Greedy Selection of Multi-Step Observations}

%\textbf{Obs}: The set of observation sequences in one's dataset 
%
%\textbf{SubObs} :All possible substrings of sequences in Obs
%
%\textbf{NumOps}: The number of operators included in $\Sigma'$. I.e NumOps =  $|\Sigma'|$
%
%Here we present a greedy heuristic which learns the multi-step transition sequences $\Sigma'$ from observation data. Having a $\Sigma'$ which reflects the types of observations produces by one's system will allow of short encodings when coupled with the encoding algorithm. In practice, this greedy algorithm will pick substrings from one's observation set which are long, frequent, and diverse. From an intuitive standpoint, one can view structure in observation sequences as relating to the level of entropy in the system's observations. 
%
%As a preprocessing step, we reduce the space of substrings textbf{subObs} to the k most frequent substrings in our observation set. Here frequent means the number of observation sequences a given substring $s \in subObs$ occurs in. 
%
%The algorithm evaluates substrings by how much they reduce the number of transition operators used on the observation data. The algorithm adds the best operator iteratively with $\Sigma'$ initialized to $\Sigma$. More formally at the i'th iteration of the algorithm the following is computed: $min_{sub \in SubObs} \sum\nolimits_{obs \in Obs}|\kappa(obs,\Sigma'_i \cup sub)|$. The algorithm terminates after $NumOps-|\Sigma\|$ iterations. Again, we provide the pseudo code for selecting $\Sigma'$ in the appendix.

\begin{algorithm}
\caption{Base Selection Algorithm}
\label{Base Selection Algorithm}
\begin{algorithmic}[1]
\INPUT $Train$, $Sub_M$
\OUTPUT $\Sigma'$

\Procedure{Base Selection}{}
\State $\Sigma' \gets \{s, s \in \sum \}$

\State $bestEncoding \gets null$
\For{each obs in Train}
	\State $bestEncoding[obs] \gets |obs|$
\EndFor

\State $i \gets 0$\
\While{$i<numOps$}
	\State $bestOp \gets null$
	\State $m \gets 0$
	\For{each s $\in Sub_M$ }
		\State $c \gets 0$
		\For{each obs in Train}
			\State $c \gets c+DPEncode(obs, \Sigma' \cup s)-bestEncoding[obs]$
		\EndFor
		
		\If{$c<m$}
			\State $bestOp \gets s$
			\State $m \gets c$
		\EndIf
		
	\EndFor

	\State $\Sigma' \gets \Sigma' \cup bestOp$
	\For{each obs in Train}
		\State $bestEncoding[obs] \gets DPEncode(obs,\Sigma'$) 
	\EndFor	
	
	\State $i \gets i + 1$
\EndWhile 
\State \Return $\Sigma'$

\EndProcedure
\end{algorithmic}
\end{algorithm}


Selecting multi-step transition sequences to build $\Sigma'$ can be achieved with an adaptive greedy algorithm, depicted in Algorithm 2. A $\Sigma'$ that correctly captures the frequent observations produced by a target environment should promote short encodings when coupled with the coding function $\kappa$ described above. In practice, we want $\Sigma'$ to contain substrings appearing in the training data which are long, frequent, and diverse. From an intuitive standpoint, one can view structure in observation sequences as relating to the level of entropy in the distribution over multi-step observations produced by the system, and thus interpret this approach as a data compression scheme. 

In general terms, the algorithm works as follows. In a preprocessing step, we find all possible substrings in $\sstar$ that appear in the training dataset. For computational reasons this can be constrained only to the $M$ most frequent substrings, where $M$ is a parameter chosen by the user. Frequency of occurence is measured by number of training trajectories that contain a given substring. The construction of $\Sigma'$ is then initialised by $\Sigma$ and proceeds iteratively by adding a new multi-step symbol at each phase. A phase starts by evaluating all substrings in terms of how much reduction in the number of transition operators used to encode the whole training set would be achieved if the symbol was added to $\Sigma'$. The algorithm then adds to $\Sigma'$ the multi-step symbol corresponding to the best substring, i.e.\ the one that would reduce the most the whole coding cost (with respect to $\kappa$). More formally, at the $i$th iteration the algorithm finds:
\begin{equation*}
\argmin_{u \in \mathrm{sub}_M} \sum_{x \in \mathrm{train}} |\kappa_{\Sigma_i' \cup \{u\}}(x)| \enspace,
\end{equation*}
where $\mathrm{train}$ is the training set, $\mathrm{sub}_M$ is the set of substrings under consideration of length at most $M$, $\Sigma_i'$ is the set of multi-step observations at the beginning of phase $i$ and we use $\kappa_{\Sigma'}$ to denote the encoding function with respect to a given set of multi-step observations for clarity. The algorithm terminates after $\Sigma'$ reaches a predetermined size.



%\borja{TODO: Introduce the $\mathrm{train}$ and $\mathrm{sub}_M$ notation used above before, in the text}
