\section{The Multi-Step PSR}

A linear \emph{predictive state representation} (PSR) for an autonomous dynamical system with discrete observations is a tuple $\psrA = \psrsigma$ where: $\Sigma$ is a finite set of possible observations, $\aone, \ainf \in \Rset^n$ are vectors of initial and final weights, and $\A_\sigma \in \Rset^{n \times n}$ are the transition operators associated with each possible observation. The dimension $n$ is the number of states of $\psrA$. Formally, a PSR is a \emph{weighted finite automata} (WFA) \cite{BLA} computing a function given by the probability distribution of sequences of observations in a partially observable dynamical system with finite state. The function $f_{\psrA} : \Sigma^\star \to \Rset$ computed by $\psrA$ is given by
\begin{equation*}
f_{\psrA}(x) = f_{\psrA}(x_1 \cdots x_t) = \aone^\top \A_{x_1} \cdots \A_{x_t} \ainf = \aone^\top \A_x \ainf \enspace.
\end{equation*}
%The value of $f_{\psrA}(x)$ is interpreted as the probability that the system produces the sequence of observations $x = x_1 \cdots x_t$ starting from the initial state specified by $\aone$. Note that one can also perform conditional queries. 
%
%\begin{equation*}
%f(\sigma^m|\sigma^n) = \dfrac{\alpha_{\lambda} A(\kappa(\sigma^m)) \alpha_\infty}{\alpha_\lambda (I-A_{\sigma})^{-1}  \alpha_{\infty} }
%\end{equation*}
%
%\begin{equation*}
%f(\sigma_1^{m_1} \sigma_2^{m_2} \cdots \sigma_k^{m_k}|\sigma_1^{n_1} \sigma_2^{n_2} \cdots \sigma_k^{n_k}) = \dfrac{\alpha_{\lambda}  A(\kappa(\sigma_1^{m_1}\sigma_2^{m_2}\cdots\sigma_k^{m_k})) \alpha_{\infty}}{\alpha_\lambda  A(\kappa(\sigma_1^{n_1}\sigma_2^{n_2}\cdots\sigma_k^{n_k}))  \alpha_{\infty} }
%\end{equation*}
%\lucas{Need to add explanation, since interpretation of f(x) can be different. Also using A(k(x)) as a notation for going from encoding to matrix product}
The value of $f_{\psrA}(x)$ is interpreted as the probability that the system produces the sequence of observations $x = x_1 \cdots x_t$ starting from the initial state specified by $\aone$. Depending on the model, this can be a probability that the system generates $x$ and \emph{stops}, or the probability that the system generates $x$ and \emph{continues}.
%
Given a partial history $u$ of the evolution of the system -- i.e.\ a prefix in string terminology -- the state of a PSR can be updated from $\aone$ to $\alphavec_u = \aone \A_u$. This allows for conditional queries about the possible observations that will follow $u$. For example, the probability of observing the string $v$ given that we have already observed $u$ will be written as
\begin{equation*}
f_{\psrA,u}(v) = \frac{\alphavec_u \A_v \ainf}{\nu_{\psrA}(u)} \enspace,
\end{equation*}
where $\nu_{\psrA}(u)$ is a normalising constant to obtain a proper conditional distribution whose expression depends on the actual semantics of the PSR (stop v.s.\ continuation probabilities).

To define our model for multi-step PSR we basically augment a PSR with two extra objects: a set of \emph{multi-step observations} $\Sigma' \subset \Sigma^+$ containing non-empty strings formed by basic observations, and a \emph{coding function} $\kappa : \Sigma^\star \to {\Sigma'}^{\star}$ that given a string of basic observations produces an equivalent string composed using multi-step observations.
%
The choice of $\Sigma'$ and $\kappa$ can be quite application-dependent, in order to reflect the particular patterns arising from different environments. However, we assume this objects satisfy a basic set of requirements for the sake of simplicity and to avoid degenerate situations:
\begin{enumerate}
\item The set $\Sigma'$ must contain all symbols in $\Sigma$; i.e.\ $\Sigma \subseteq \Sigma'$
\item The function $\kappa$ satisfies $\partial(\kappa(x)) = x$ for all $x \in \Sigma^\star$, where $\partial : {\Sigma'}^\star \to \Sigma^\star$ is the \emph{decoding morphism} between free monoids given by $\partial(z) = z \in \Sigma^\star$ for all $z \in \Sigma'$. Note this implies that $\kappa(\epsilon) = \epsilon$, $\kappa(\sigma) = \sigma$ for all $\sigma \in \Sigma$, and $\kappa$ is injective.
\end{enumerate}

Using these definitions, a \emph{multi-step PSR} (M-PSR) is a tuple $\psrA' = \mpsrsigma$ containing a PSR with observations in $\Sigma'$, together with the basic observations $\Sigma$ and the corresponding coding function $\kappa$. In addition to the function $f_{\psrA'} : {\Sigma'}^\star \to \R$ that $\psrA'$ by being a PSR over $\Sigma'$, it also computes another function $f_{\psrA'}' : \sstar \to \R$ given by $f_{\psrA'}'(x) = f_{\psrA'}(\kappa(x))$. In many cases we will abuse our notation and write $f_{\psrA'}$ for $f_{\psrA'}'$ when there is no risk of confusion.

%\borja{For now, this is enough}

\subsection{Examples of M-PSRs}

We now describe several examples of M-PSR, and put special emphasis on models that will be used in our experiments.

\subsubsection{Base M-PSR}
%A PSR with a single observation $\Sigma = \{a\}$ can be used to measure the time -- i.e.\ number of discrete time-steps -- until a certain event happens \cite{ODM}. In this case, a natural approach to build an M-PSR for timing models is to build a set of multi-step observations containing sequences whose lengths are powers of a fixed base. That is, given an integer $b > 0$, we build the set of multi-step observations as $\Sigma' = \{a,a^b, a^{b^2}, \ldots, a^{b^K}\}$ for some positive $K$. A natural choice of coding map in this case is the one that represents any length $t$ as a number in base $b$, with the difference that the largest power $b$ that is allowed is $b^K$. This corresponds to writing (in a unique way) $t = t_0 b^0 + t_1 b^1 + t_2 b^2 + \cdots + t_K b^K$, where $0 \leq t_k \leq b - 1$ for $0 \leq k \leq K-1$, and $t_K \geq 0$. With this decomposition we obtain the coding map $\kappa(a^t) = (a^{b^K})^{t_K} (a^{b^{K-1}})^{t_{K-1}} \cdots (a^b)^{t_1} (a)^{t_0}$. Note that we choose to write powers of longer multi-step observations first, followed by powers of shorter multi-step observations. For further reference, we will call this model the Base M-PSR.. 
A PSR with a single observation $\Sigma = \{\sigma\}$ can be used to measure the time -- i.e.\ number of discrete time-steps -- until a certain event happens \cite{ODM}. In this case, a natural approach to build an M-PSR for timing models is to build a set of multi-step observations containing sequences of $a$'s whose lengths are powers of a fixed base. That is, given an integer $b > 0$, we build the set of multi-step observations as $\Sigma' = \{\sigma,\sigma^b, \sigma^{b^2}, \ldots, \sigma^{b^K}\}$ for some positive $K$. A natural choice of coding map in this case is the one that represents any length $t \geq 0$ as a number in base $b$, with the difference that the largest power $b$ that is allowed is $b^K$. This corresponds to writing (in a unique way) $t = t_0 b^0 + t_1 b^1 + t_2 b^2 + \cdots + t_K b^K$, where $0 \leq t_k \leq b - 1$ for $0 \leq k \leq K-1$, and $t_K \geq 0$. With this decomposition we obtain the coding map $\kappa(\sigma^t) = (\sigma^{b^K})^{t_K} (\sigma^{b^{K-1}})^{t_{K-1}} \cdots (\sigma^b)^{t_1} (\sigma)^{t_0}$. Note that we choose to write powers of longer multi-step observations first, followed by powers of shorter multi-step observations. For further reference, we will call this model the Base M-PSR.

%\lucas{changed K-1 to K}
%\borja{changed it again: $t_K$ has no upper bound}

%Borja's ending:
%For further reference, we will call this model the %\emph{base M-PSR} (with base $b$ and largest power $K$) %for modelling distributions over time.

%\borja{Re-wrote the presentation of the base M-PSR a little bit}

%For the multiple observation case one can also use a Base M-PSR. In this case $\Sigma' = \{\sigma_1,\sigma_2,\sigma_1^b,\sigma_2^{b},\sigma_1^{b^2},\sigma_2^{b^2}, ...,\sigma_1^{b^k},\sigma_2^{b^k}\}$. Here $\sigma_1$ and $\sigma_2$ are two observations symbols' this can of course be extended for any finite number of observations. For the encoding map $\kappa$ we first split the string into sequences of a fixed symbol and then use the same encoding as for timing. As an example: $\kappa(\sigma_1^5 \sigma_2^3)=(\sigma_1^{4})(\sigma_1)(\sigma_2^2)(\sigma_2)$.
For the multiple observation case one can also use a Base M-PSR. For example, if there are two observations $\Sigma = \{\sigma_1, \sigma_2\}$, we can take $\Sigma' = \{\sigma_1,\sigma_2,\sigma_1^b,\sigma_2^{b},\sigma_1^{b^2},\sigma_2^{b^2}, ...,\sigma_1^{b^K},\sigma_2^{b^K}\}$. This can of course be extended for any finite number of observations. For the encoding map $\kappa$ we first split the string into sequences of consecutive repeated symbols and then use the same encoding as for timing. For example: $\kappa(\sigma_1^5 \sigma_2^3)=(\sigma_1^2)^2(\sigma_1)(\sigma_2^2)(\sigma_2)$ when using $b = 2$ and $K = 1$.

\subsubsection{Tree M-PSR}
 
%Another example for the multiple observation case is what we call the Tree M-PSR. For the Tree M-PSR, we set $\Sigma'= \{s \in \Sigma^\star, len(s)<= L\}$. Here L is a parameter of choice, but note that $|\Sigma'|={|\Sigma|}^L$, thus in practice L must remain small as learning operators is computationally intensive. For the decoding map $\kappa$, we first split a string x as $x=x_1x_2...x_nx_f$, where $len(x_i)==L, \forall i<=n$ and $len(x_f)=len(x)-(n \cdot L)$. With this we set $\kappa(x) = \{x_1,x_2,...,x_n,x_f\}$.
Another example for the multiple observation case is what we call the Tree M-PSR. In a Tree M-PSR, we set $\Sigma'= \{x \in \Sigma^\star, |x| \leq L\}$. Here $L$ is a parameter of choice, but note that $|\Sigma'|= O({|\Sigma|}^L)$, thus in practice $L$ must remain small if we want the M-PSR to be representable using a small number of parameters. For the decoding map $\kappa$, we first split a string $x$ as $x=u_1 u_2 \cdots u_m u_f$, where $|u_i| = L$ for $1 \leq i \leq n$ and $|u_f| =|x|-(m \cdot L) < L$. With this we set $\kappa(x) = (u_1)(u_2) \cdots (u_m) (u_f)$. Note that this encoding promotes the use of multi-step symbols corresponding to longer strings more often that those symbols corresponding to shorter strings.

%\borja{LUCAS: I see an inconsistency here. If you add to $\Sigma'$ strings of length $1,2,\ldots,L$, then once you're done using the symbols of length $L$ in $\kappa$ you should start using symbols of length $L-1$, and so on. But the text seems to suggest that you jump from symbols of length $L$ to symbols of length $1$, in which case it'd be better to choose $\Sigma' = \Sigma \cup \Sigma^L$ directly. Note I changed some of your notation when making edits.}
%
%\lucas{I don't see where the inconsistency is. Maybe i'm missing something but $u_f$ is just the string remainder $|u_f|$ < L, so we don't jump from L to 1. Ex: L = 3 "abcab" -> ("abc")("ab")}
%\borja{understood, corrected the wording}

\subsubsection{Data-Driven M-PSR}

%The constructions of the M-PSRs above are not dependent on the environment. In our experiments section, we find that performance of M-PSRs depend heavily on how $\Sigma'$ reflects the observations in one's environment. Thus, we develop an algorithm for choosing $\Sigma'$. In addition, we provide a $\kappa$ which one can apply to any M-PSR and which delivers good experimental performance. Together, these yield another type of M-PSR, which we call the Data-Driven M-PSR.

Although the constructions above have some parameters that can be tuned depending on the particular application, they are not directly dependent on the observations produced by the target environment. In our experiments section, we find that performance of M-PSRs depend heavily on how $\Sigma'$ reflects the observations in from the target environment. Thus, we develop an algorithm for choosing $\Sigma'$ in a data-driven way. In addition, we provide a generic coding function $\kappa$ that can be applied to any M-PSR. This encoding delivers good experimental predictive performance and computationally inexpensive probability queries. Together, these yield another type of M-PSR, which we call the Data-Driven M-PSR, which is the output of the learning algorithm described in the next section.

%\lucas{Split one long sentence into two shorter ones}


